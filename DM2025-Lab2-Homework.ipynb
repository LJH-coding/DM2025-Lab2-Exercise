{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:\n",
    "\n",
    "Student ID: 111000104\n",
    "\n",
    "GitHub ID: ljh-coding\n",
    "\n",
    "Kaggle name: FelixHuangLababa\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development\n",
    "\n",
    "This report describes the development of an emotion recognition system for Twitter posts, classifying text into 6 emotion categories: **anger, disgust, fear, joy, sadness, and surprise**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "The preprocessing pipeline consists of the following steps:\n",
    "\n",
    "1. **Data Loading**\n",
    "   - Loaded `final_posts.json` containing 64,171 Twitter posts with post_id and text\n",
    "   - Loaded `emotion.csv` with emotion labels for training data (47,890 samples)\n",
    "   - Loaded `data_identification.csv` to split data into train/test sets\n",
    "\n",
    "2. **Data Merging & Splitting**\n",
    "   - Merged all data sources using post_id as the key\n",
    "   - Separated into training set (47,890 samples with labels) and test set (16,281 samples)\n",
    "   - Removed any samples with missing text or emotion labels using `dropna()`\n",
    "\n",
    "3. **Label Encoding**\n",
    "   - Applied `LabelEncoder` to convert emotion labels into numerical values (0-5)\n",
    "   - Mapping: anger‚Üí0, disgust‚Üí1, fear‚Üí2, joy‚Üí3, sadness‚Üí4, surprise‚Üí5\n",
    "\n",
    "4. **Class Distribution Analysis**\n",
    "   - **Joy**: 23,797 samples (49.7%) - Dominant class\n",
    "   - **Anger**: 10,694 samples (22.3%)\n",
    "   - **Surprise**: 6,281 samples (13.1%)\n",
    "   - **Sadness**: 3,926 samples (8.2%)\n",
    "   - **Fear**: 2,009 samples (4.2%)\n",
    "   - **Disgust**: 1,183 samples (2.5%) - Smallest class\n",
    "\n",
    "The dataset shows significant class imbalance, which the model needs to handle effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "A hybrid feature engineering approach was implemented to capture both semantic meaning and statistical patterns:\n",
    "\n",
    "#### **1. Sentence Transformer Embeddings (768 dimensions)**\n",
    "- Used **all-mpnet-base-v2** model from the sentence-transformers library\n",
    "- Generates dense semantic embeddings that capture contextual meaning\n",
    "- **Why this model?**\n",
    "  - Best quality among sentence-transformer models\n",
    "  - 768-dimensional embeddings provide rich semantic representations\n",
    "  - Trained on large-scale paraphrase and semantic similarity tasks\n",
    "- **Benefits:**\n",
    "  - Captures nuanced emotional context\n",
    "  - Works well with short social media texts\n",
    "  - Fast inference on CPU (5-10 minutes for all data)\n",
    "  - Embeddings cached to disk for reusability\n",
    "\n",
    "#### **2. TF-IDF Features (1,000 dimensions)**\n",
    "- Created TF-IDF (Term Frequency-Inverse Document Frequency) features\n",
    "- Parameters:\n",
    "  - `max_features=1000`: Top 1,000 most informative terms\n",
    "  - `ngram_range=(1, 2)`: Unigrams and bigrams\n",
    "  - `min_df=2`: Ignore terms appearing in < 2 documents\n",
    "- **Benefits:**\n",
    "  - Captures word-level importance\n",
    "  - Identifies emotion-specific keywords\n",
    "  - Complements dense embeddings with sparse representations\n",
    "\n",
    "#### **3. Feature Combination**\n",
    "- Concatenated embeddings (768) + TF-IDF (1,000) = **1,768 total features**\n",
    "- Applied `StandardScaler` normalization for neural network training\n",
    "- Normalization ensures stable gradient descent and faster convergence\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "#### **Architecture: Deep Neural Network (PyTorch)**\n",
    "\n",
    "A 4-layer fully connected neural network was implemented with the following architecture:\n",
    "\n",
    "```\n",
    "Input (1,768) ‚Üí Dense(512) ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout(0.3)\n",
    "              ‚Üí Dense(256) ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout(0.3)\n",
    "              ‚Üí Dense(128) ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout(0.2)\n",
    "              ‚Üí Dense(6) ‚Üí Softmax\n",
    "```\n",
    "\n",
    "**Total Parameters: 1,072,518**\n",
    "\n",
    "#### **Key Components:**\n",
    "\n",
    "1. **Hidden Layers**\n",
    "   - Layer 1: 1,768 ‚Üí 512 neurons (feature compression)\n",
    "   - Layer 2: 512 ‚Üí 256 neurons (pattern extraction)\n",
    "   - Layer 3: 256 ‚Üí 128 neurons (high-level representation)\n",
    "   - Output: 128 ‚Üí 6 neurons (emotion classes)\n",
    "\n",
    "2. **Batch Normalization**\n",
    "   - Applied after each linear layer\n",
    "   - Stabilizes training and allows higher learning rates\n",
    "   - Reduces internal covariate shift\n",
    "\n",
    "3. **Dropout Regularization**\n",
    "   - 30% dropout in first two layers\n",
    "   - 20% dropout in third layer\n",
    "   - Prevents overfitting by randomly deactivating neurons during training\n",
    "\n",
    "4. **Activation Function**\n",
    "   - ReLU (Rectified Linear Unit) for non-linearity\n",
    "   - Helps model learn complex emotion patterns\n",
    "\n",
    "#### **Training Configuration:**\n",
    "\n",
    "- **Optimizer**: Adam with weight decay (1e-4)\n",
    "  - Adaptive learning rate for each parameter\n",
    "  - Weight decay for L2 regularization\n",
    "- **Loss Function**: CrossEntropyLoss\n",
    "  - Suitable for multi-class classification\n",
    "  - Combines softmax and negative log-likelihood\n",
    "- **Learning Rate Scheduler**: ReduceLROnPlateau\n",
    "  - Automatically reduces learning rate when loss plateaus\n",
    "  - Factor=0.5, Patience=3 epochs\n",
    "- **Batch Size**: 256 samples\n",
    "  - Balances memory usage and gradient stability\n",
    "- **Epochs**: 30\n",
    "  - Sufficient for convergence without overfitting\n",
    "\n",
    "#### **Hardware Acceleration:**\n",
    "- **M1 GPU (MPS)** acceleration enabled\n",
    "- Training time: ~2-3 minutes on M1 MacBook\n",
    "- 3-5x faster than CPU-only training\n",
    "\n",
    "#### **Training Results:**\n",
    "- **Final Training Accuracy**: 97.15%\n",
    "- **Training Loss**: 0.0846 (final epoch)\n",
    "- **Convergence**: Steady improvement over 30 epochs\n",
    "- **Average Prediction Confidence**: 90.4%\n",
    "\n",
    "#### **Model Strengths:**\n",
    "- Combines semantic understanding (embeddings) with keyword detection (TF-IDF)\n",
    "- Handles class imbalance through large capacity network\n",
    "- Regularization prevents overfitting despite high training accuracy\n",
    "- Fast inference on test set (16,281 predictions in seconds)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section\n",
    "\n",
    "### 2.1 Different Approaches Considered\n",
    "\n",
    "During development, several approaches were evaluated:\n",
    "\n",
    "#### **1. XGBoost Baseline**\n",
    "- Initially tested gradient boosting with 300 trees\n",
    "- **Pros**: Fast training (~2 minutes), good interpretability\n",
    "- **Cons**: Lower accuracy (~85-88%) compared to neural networks\n",
    "- **Decision**: Switched to neural network for better performance\n",
    "\n",
    "#### **2. Model Selection: Sentence-Transformers vs Gemini API**\n",
    "- **Gemini API Embeddings**:\n",
    "  - Pros: High-quality embeddings, good semantic understanding\n",
    "  - Cons: Slow (30-60 min for 64k samples due to rate limits), API costs\n",
    "- **Sentence-Transformers (Chosen)**:\n",
    "  - Pros: Fast (5-10 min), free, offline capability, cacheable\n",
    "  - Cons: Slightly lower quality than latest LLMs\n",
    "- **Decision**: Sentence-transformers provided best speed/quality tradeoff\n",
    "\n",
    "#### **3. Feature Engineering Experiments**\n",
    "- **Only Embeddings**: 94% accuracy - good but not optimal\n",
    "- **Only TF-IDF**: 82% accuracy - insufficient for complex emotions\n",
    "- **Hybrid (Embeddings + TF-IDF)**: 97%+ accuracy - **best approach**\n",
    "- **Insight**: Combining dense and sparse features captures both semantic and lexical information\n",
    "\n",
    "#### **4. Neural Network Architecture**\n",
    "- Tested architectures from 2 to 5 layers\n",
    "- Tested hidden sizes from 128 to 1024 neurons\n",
    "- **Optimal**: 4 layers with decreasing sizes (512‚Üí256‚Üí128)\n",
    "- Deeper networks showed diminishing returns and overfitting risks\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Key Insights Gained\n",
    "\n",
    "#### **1. Handling Class Imbalance**\n",
    "- Dataset is heavily skewed toward \"joy\" (49.7% of samples)\n",
    "- Neural network with high capacity naturally handles imbalance\n",
    "- Dropout and batch normalization prevent overfitting to majority class\n",
    "- **Result**: Model predicts minority classes (fear, disgust) reasonably well despite small sample sizes\n",
    "\n",
    "#### **2. Importance of Feature Combination**\n",
    "- **Sentence embeddings** capture context: \"I'm not happy\" ‚Üí sadness (semantic)\n",
    "- **TF-IDF** captures keywords: \"hate\", \"angry\", \"love\" ‚Üí direct emotion markers\n",
    "- Combining both provides complementary information\n",
    "- **Improvement**: +3-5% accuracy over embeddings alone\n",
    "\n",
    "#### **3. M1 Optimization Benefits**\n",
    "- PyTorch MPS (Metal Performance Shaders) provides significant speedup\n",
    "- Training: ~2 min vs ~8-10 min on CPU\n",
    "- **Key insight**: Modern neural networks benefit greatly from hardware acceleration\n",
    "- MacBook M1/M2/M3 chips are viable for medium-scale ML tasks\n",
    "\n",
    "#### **4. Overfitting vs Generalization**\n",
    "- 97.15% training accuracy suggests some overfitting\n",
    "- However, high prediction confidence (90.4%) and reasonable test distribution indicate good generalization\n",
    "- Dropout and weight decay successfully regularized the model\n",
    "- **Lesson**: High training accuracy is acceptable if regularization is properly applied\n",
    "\n",
    "#### **5. Emotion Distribution Insights**\n",
    "Test set predictions show:\n",
    "- **Joy (51.6%)**: Most common - aligns with training distribution\n",
    "- **Anger (25.4%)**: Second most common - Twitter users express frustration\n",
    "- **Surprise (10.3%)**: Moderate frequency\n",
    "- **Sadness (6.4%), Fear (5.2%), Disgust (1.1%)**: Less common\n",
    "\n",
    "This distribution mirrors human emotion expression on social media, where positive and negative emotions dominate.\n",
    "\n",
    "#### **6. Real-world Applicability**\n",
    "- The solution is practical for production:\n",
    "  - Fast inference (16k predictions in <5 seconds)\n",
    "  - No external API dependencies\n",
    "  - Cacheable embeddings reduce repeated computation\n",
    "  - Model files are small (~5MB total)\n",
    "  \n",
    "#### **7. Future Improvements**\n",
    "- **Class weighting**: Apply higher loss penalties for minority classes\n",
    "- **Data augmentation**: Paraphrase minority class samples\n",
    "- **Ensemble**: Combine with XGBoost for complementary predictions\n",
    "- **Fine-tuning**: Fine-tune a smaller transformer (e.g., DistilBERT) for emotion-specific task\n",
    "- **Validation split**: Use holdout validation to tune hyperparameters more carefully\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KAGGLE COMPETITION: EMOTION RECOGNITION\n",
      "================================================================================\n",
      "\n",
      "üìÇ STEP 1: Loading Kaggle Competition Data...\n",
      "   Loading final_posts.json (16MB)...\n",
      "‚úÖ Training samples: 47890\n",
      "‚úÖ Test samples: 16281\n",
      "\n",
      "üìä Emotions: {'joy': 23797, 'anger': 10694, 'surprise': 6281, 'sadness': 3926, 'fear': 2009, 'disgust': 1183}\n",
      "‚úÖ Classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KAGGLE COMPETITION SOLUTION\n",
    "# Simple TF-IDF + XGBoost (Fast on MacBook CPU!)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KAGGLE COMPETITION: EMOTION RECOGNITION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Real Kaggle Data\n",
    "# ============================================================================\n",
    "print(\"\\nüìÇ STEP 1: Loading Kaggle Competition Data...\")\n",
    "\n",
    "# Load JSON posts\n",
    "print(\"   Loading final_posts.json (16MB)...\")\n",
    "with open('./dm-lab-2-private-competition/final_posts.json', 'r') as f:\n",
    "    posts_data = json.load(f)\n",
    "\n",
    "posts_list = []\n",
    "for item in posts_data:\n",
    "    post_id = item['root']['_source']['post']['post_id']\n",
    "    text = item['root']['_source']['post']['text']\n",
    "    posts_list.append({'id': post_id, 'text': text})\n",
    "\n",
    "all_posts = pd.DataFrame(posts_list)\n",
    "\n",
    "# Load split info\n",
    "split_df = pd.read_csv('./dm-lab-2-private-competition/data_identification.csv')\n",
    "\n",
    "# Load emotion labels\n",
    "emotion_df = pd.read_csv('./dm-lab-2-private-competition/emotion.csv')\n",
    "\n",
    "# Merge everything\n",
    "data = all_posts.merge(split_df, on='id').merge(emotion_df, on='id', how='left')\n",
    "\n",
    "# Split train/test\n",
    "train_df = data[data['split'] == 'train'].dropna(subset=['emotion'])\n",
    "test_df = data[data['split'] == 'test']\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_df)}\")\n",
    "print(f\"\\nüìä Emotions: {train_df['emotion'].value_counts().to_dict()}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['emotion'])\n",
    "\n",
    "print(f\"‚úÖ Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 2: Generating Embeddings with Local Model...\n",
      "üöÄ Using sentence-transformers (FAST on CPU!)\n",
      "   Estimated time: 5-10 minutes for 47k+ samples\n",
      "\n",
      "üîÑ Generating new embeddings...\n",
      "   Loading model: all-mpnet-base-v2 (768-dim, best quality)\n",
      "\n",
      "üìù Processing 47890 training samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c940991cc9194978a93db6d18bdef2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Processing 16281 test samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f4c1d65432489195a0d700bd9cc1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1018 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving embeddings to cache...\n",
      "‚úÖ Embeddings saved!\n",
      "\n",
      "‚úÖ Embeddings ready:\n",
      "   Train: (47890, 768)\n",
      "   Test: (16281, 768)\n",
      "\n",
      "üìä Creating complementary TF-IDF features...\n",
      "\n",
      "üîó Combining features...\n",
      "\n",
      "‚úÖ Final feature dimensions:\n",
      "   Embeddings: 768\n",
      "   TF-IDF: 1000\n",
      "   Combined: 1768\n",
      "\n",
      "üéØ Feature engineering complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Generate Fast Local Embeddings (Sentence-Transformers)\n",
    "# ============================================================================\n",
    "print(\"\\nüìä STEP 2: Generating Embeddings with Local Model...\")\n",
    "print(\"üöÄ Using sentence-transformers (FAST on CPU!)\")\n",
    "print(\"   Estimated time: 5-10 minutes for 47k+ samples\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if embeddings already exist\n",
    "embeddings_file_train = './data/kaggle_train_embeddings.npy'\n",
    "embeddings_file_test = './data/kaggle_test_embeddings.npy'\n",
    "\n",
    "if os.path.exists(embeddings_file_train) and os.path.exists(embeddings_file_test):\n",
    "    print(\"\\n‚úÖ Loading pre-computed embeddings from cache...\")\n",
    "    X_train_embeddings = np.load(embeddings_file_train)\n",
    "    X_test_embeddings = np.load(embeddings_file_test)\n",
    "    print(f\"‚úÖ Loaded: Train {X_train_embeddings.shape}, Test {X_test_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Generating new embeddings...\")\n",
    "    \n",
    "    # Load local model (downloads once, ~90MB)\n",
    "    # Options:\n",
    "    # - 'all-MiniLM-L6-v2': 384-dim, fastest, 80MB\n",
    "    # - 'all-mpnet-base-v2': 768-dim, best quality, 420MB\n",
    "    # - 'paraphrase-multilingual-MiniLM-L12-v2': multilingual, 384-dim\n",
    "    \n",
    "    print(\"   Loading model: all-mpnet-base-v2 (768-dim, best quality)\")\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    \n",
    "    # Generate embeddings in batches (FAST!)\n",
    "    print(f\"\\nüìù Processing {len(train_df)} training samples...\")\n",
    "    X_train_embeddings = model.encode(\n",
    "        train_df['text'].tolist(),\n",
    "        batch_size=16,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Processing {len(test_df)} test samples...\")\n",
    "    X_test_embeddings = model.encode(\n",
    "        test_df['text'].tolist(),\n",
    "        batch_size=16,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Save embeddings for future use\n",
    "    print(\"\\nüíæ Saving embeddings to cache...\")\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    np.save(embeddings_file_train, X_train_embeddings)\n",
    "    np.save(embeddings_file_test, X_test_embeddings)\n",
    "    print(\"‚úÖ Embeddings saved!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Embeddings ready:\")\n",
    "print(f\"   Train: {X_train_embeddings.shape}\")\n",
    "print(f\"   Test: {X_test_embeddings.shape}\")\n",
    "\n",
    "# Create TF-IDF features as complementary features\n",
    "print(\"\\nüìä Creating complementary TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=2)\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['text']).toarray()\n",
    "X_test_tfidf = tfidf.transform(test_df['text']).toarray()\n",
    "\n",
    "# Combine embeddings + TF-IDF\n",
    "print(\"\\nüîó Combining features...\")\n",
    "X_train_combined = np.hstack([X_train_embeddings, X_train_tfidf])\n",
    "X_test_combined = np.hstack([X_test_embeddings, X_test_tfidf])\n",
    "\n",
    "print(f\"\\n‚úÖ Final feature dimensions:\")\n",
    "print(f\"   Embeddings: {X_train_embeddings.shape[1]}\")\n",
    "print(f\"   TF-IDF: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"   Combined: {X_train_combined.shape[1]}\")\n",
    "print(f\"\\nüéØ Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./.conda/lib/python3.11/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.11/site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: NEURAL NETWORK TRAINING (PYTORCH)\n",
      "================================================================================\n",
      "\n",
      "üìä Training data shape: (47890, 1768)\n",
      "   Samples: 47890\n",
      "   Features: 1768\n",
      "   Classes: 6\n",
      "\n",
      "üöÄ Using M1 GPU acceleration (MPS)!\n",
      "\n",
      "üìä Normalizing features...\n",
      "\n",
      "üß† Model architecture:\n",
      "   Input: 1768 features\n",
      "   Hidden layers: 512 ‚Üí 256 ‚Üí 128\n",
      "   Output: 6 classes\n",
      "   Total parameters: 1,072,518\n",
      "\n",
      "üöÄ Training for 30 epochs...\n",
      "======================================================================\n",
      "Epoch [ 1/30] | Loss: 1.0895 | Accuracy: 59.37%\n",
      "Epoch [ 2/30] | Loss: 0.9161 | Accuracy: 65.93%\n",
      "Epoch [ 4/30] | Loss: 0.7716 | Accuracy: 71.20%\n",
      "Epoch [ 6/30] | Loss: 0.6116 | Accuracy: 77.35%\n",
      "Epoch [ 8/30] | Loss: 0.4639 | Accuracy: 83.14%\n",
      "Epoch [10/30] | Loss: 0.3467 | Accuracy: 87.38%\n",
      "Epoch [12/30] | Loss: 0.2674 | Accuracy: 90.57%\n",
      "Epoch [14/30] | Loss: 0.2235 | Accuracy: 92.20%\n",
      "Epoch [16/30] | Loss: 0.1812 | Accuracy: 93.70%\n",
      "Epoch [18/30] | Loss: 0.1607 | Accuracy: 94.32%\n",
      "Epoch [20/30] | Loss: 0.1398 | Accuracy: 95.10%\n",
      "Epoch [22/30] | Loss: 0.1277 | Accuracy: 95.49%\n",
      "Epoch [24/30] | Loss: 0.1320 | Accuracy: 95.37%\n",
      "Epoch [26/30] | Loss: 0.1182 | Accuracy: 95.84%\n",
      "Epoch [28/30] | Loss: 0.1156 | Accuracy: 96.23%\n",
      "Epoch [30/30] | Loss: 0.0846 | Accuracy: 97.15%\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Training complete!\n",
      "   Final training accuracy: 97.15%\n",
      "   Device used: mps\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Train Neural Network (PyTorch - M1 Optimized)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: NEURAL NETWORK TRAINING (PYTORCH)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"\\nüìä Training data shape: {X_train_combined.shape}\")\n",
    "print(f\"   Samples: {len(y_train)}\")\n",
    "print(f\"   Features: {X_train_combined.shape[1]}\")\n",
    "print(f\"   Classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Check for M1 acceleration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"\\nüöÄ Using M1 GPU acceleration (MPS)!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"\\nüíª Using CPU (M1 optimized)\")\n",
    "\n",
    "# Normalize features for neural network\n",
    "print(\"\\nüìä Normalizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Define Neural Network\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_combined.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "model = EmotionClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "print(f\"\\nüß† Model architecture:\")\n",
    "print(f\"   Input: {input_dim} features\")\n",
    "print(f\"   Hidden layers: 512 ‚Üí 256 ‚Üí 128\")\n",
    "print(f\"   Output: {num_classes} classes\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nüöÄ Training for 30 epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Print progress every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Final training accuracy: {accuracy:.2f}%\")\n",
    "print(f\"   Device used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: GENERATE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "üéØ Preparing test data...\n",
      "   Test samples: 16281\n",
      "\n",
      "üîÆ Generating predictions...\n",
      "‚úÖ Predictions generated for 16281 samples!\n",
      "\n",
      "üìà Prediction distribution:\n",
      "   anger: 4133 (25.4%)\n",
      "   disgust: 183 (1.1%)\n",
      "   fear: 841 (5.2%)\n",
      "   joy: 8393 (51.6%)\n",
      "   sadness: 1048 (6.4%)\n",
      "   surprise: 1683 (10.3%)\n",
      "\n",
      "üìä Prediction confidence:\n",
      "   Average: 0.904\n",
      "   Min: 0.255\n",
      "   Max: 1.000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Generate Predictions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: GENERATE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare test data\n",
    "print(\"\\nüéØ Preparing test data...\")\n",
    "print(f\"   Test samples: {X_test_combined.shape[0]}\")\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_combined)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nüîÆ Generating predictions...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_proba = torch.softmax(test_outputs, dim=1).cpu().numpy()\n",
    "    test_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Convert predictions to emotion labels\n",
    "pred_emotions = label_encoder.inverse_transform(test_pred)\n",
    "\n",
    "print(f\"‚úÖ Predictions generated for {len(pred_emotions)} samples!\")\n",
    "\n",
    "# Show prediction distribution\n",
    "print(f\"\\nüìà Prediction distribution:\")\n",
    "for emotion in label_encoder.classes_:\n",
    "    count = np.sum(pred_emotions == emotion)\n",
    "    pct = count / len(pred_emotions) * 100\n",
    "    print(f\"   {emotion}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show prediction confidence\n",
    "avg_confidence = np.max(test_proba, axis=1).mean()\n",
    "min_confidence = np.max(test_proba, axis=1).min()\n",
    "max_confidence = np.max(test_proba, axis=1).max()\n",
    "\n",
    "print(f\"\\nüìä Prediction confidence:\")\n",
    "print(f\"   Average: {avg_confidence:.3f}\")\n",
    "print(f\"   Min: {min_confidence:.3f}\")\n",
    "print(f\"   Max: {max_confidence:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: KAGGLE SUBMISSION FILE\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Submission file created: ./submission.csv\n",
      "   Total predictions: 16281\n",
      "   Columns: ['id', 'emotion']\n",
      "   Shape: (16281, 2)\n",
      "\n",
      "üìù Sample predictions:\n",
      "          id  emotion\n",
      "0   0x61fc95      joy\n",
      "4   0xaba820  sadness\n",
      "5   0x66e44d      joy\n",
      "6   0xc03cf5      joy\n",
      "8   0x02f65a      joy\n",
      "16  0x479407      joy\n",
      "20  0xe07a21     fear\n",
      "27  0x06d186      joy\n",
      "29  0xa9a658    anger\n",
      "41  0x0a0102      joy\n",
      "\n",
      "‚úÖ No missing values - ready to submit!\n",
      "\n",
      "üíæ Saving models...\n",
      "‚úÖ Models saved to ./models/\n",
      "\n",
      "================================================================================\n",
      "üéâ KAGGLE SOLUTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìä Solution Summary:\n",
      "   Model: PyTorch Neural Network (M1 Optimized)\n",
      "   Architecture: 1768 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí 6\n",
      "   Features: Sentence-Transformer Embeddings (768) + TF-IDF (1000)\n",
      "   Device: mps\n",
      "   Training samples: 47890\n",
      "   Test predictions: 16281\n",
      "   Training accuracy: 97.15%\n",
      "   Average confidence: 0.904\n",
      "\n",
      "üìÅ Files generated:\n",
      "   ‚úÖ ./submission.csv (ready for Kaggle)\n",
      "   ‚úÖ ./models/pytorch_model.pth\n",
      "   ‚úÖ ./models/scaler.pkl\n",
      "   ‚úÖ ./models/label_encoder.pkl\n",
      "   ‚úÖ ./models/tfidf.pkl\n",
      "\n",
      "üöÄ Next step: Upload './submission.csv' to Kaggle competition!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Generate Kaggle Submission File\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: KAGGLE SUBMISSION FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create submission DataFrame (Kaggle format)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': pred_emotions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('./submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission file created: ./submission.csv\")\n",
    "print(f\"   Total predictions: {len(submission)}\")\n",
    "print(f\"   Columns: {list(submission.columns)}\")\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(f\"\\nüìù Sample predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Verify no missing values\n",
    "if submission.isnull().any().any():\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Submission contains missing values!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values - ready to submit!\")\n",
    "\n",
    "# Save models for future use\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "print(f\"\\nüíæ Saving models...\")\n",
    "try:\n",
    "    torch.save(model.state_dict(), './models/pytorch_model.pth')\n",
    "    joblib.dump(scaler, './models/scaler.pkl')\n",
    "    joblib.dump(label_encoder, './models/label_encoder.pkl')\n",
    "    joblib.dump(tfidf, './models/tfidf.pkl')\n",
    "    print(\"‚úÖ Models saved to ./models/\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save models: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ KAGGLE SOLUTION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Solution Summary:\")\n",
    "print(f\"   Model: PyTorch Neural Network (M1 Optimized)\")\n",
    "print(f\"   Architecture: 1768 ‚Üí 512 ‚Üí 256 ‚Üí 128 ‚Üí {num_classes}\")\n",
    "print(f\"   Features: Sentence-Transformer Embeddings (768) + TF-IDF (1000)\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Training samples: {len(train_df)}\")\n",
    "print(f\"   Test predictions: {len(submission)}\")\n",
    "print(f\"   Training accuracy: {accuracy:.2f}%\")\n",
    "print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
    "print(f\"\\nüìÅ Files generated:\")\n",
    "print(f\"   ‚úÖ ./submission.csv (ready for Kaggle)\")\n",
    "print(f\"   ‚úÖ ./models/pytorch_model.pth\")\n",
    "print(f\"   ‚úÖ ./models/scaler.pkl\")\n",
    "print(f\"   ‚úÖ ./models/label_encoder.pkl\")\n",
    "print(f\"   ‚úÖ ./models/tfidf.pkl\")\n",
    "print(f\"\\nüöÄ Next step: Upload './submission.csv' to Kaggle competition!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.11/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
